{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ced4e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code not fully functional in this state\n",
    "# Only for display/appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7c5ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import solve_ivp\n",
    "import pysindy as ps\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import random\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86854faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian noise addition\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def add_noise(data, percentage=0):\n",
    "    rmse = mean_squared_error(u, np.zeros(data.shape), squared=False)\n",
    "    data = data + np.random.normal(0, (percentage/100)*rmse, u.shape)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8952741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Savitzky-Golay Filter \n",
    "from scipy.signal import savgol_filter\n",
    "def savgol_denoise_1D(data, window_length=5,poly_order=0):\n",
    "    denoised_data = savgol_filter(data, window_length, poly_order)  \n",
    "    return denoised_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a7899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1 Trend Filter\n",
    "import cvxpy as cp\n",
    "def l1_trend_filter(y, lamb=0):\n",
    "    n = len(y)\n",
    "    x = cp.Variable(n)\n",
    "    # Construct 2nd order difference matrix D2\n",
    "    D = np.diff(np.eye(n), axis=0)\n",
    "    D2 = np.diff(D, axis=0)\n",
    "    # Define the objective\n",
    "    objective = cp.Minimize(0.5 * cp.norm(x - y, 2)**2 + \n",
    "                            lamb * cp.norm(D2 @ x, 1))\n",
    "    # Construct the problem\n",
    "    problem = cp.Problem(objective)\n",
    "    # Apply ECOS solver\n",
    "    try:\n",
    "        problem.solve()\n",
    "    except cp.SolverError:\n",
    "        # Apply SCS solver if ECOS fails\n",
    "        try:\n",
    "            problem.solve(solver=cp.SCS)\n",
    "        # No solution found\n",
    "        except cp.SolverError:\n",
    "            return None\n",
    "    # Return the solution \n",
    "    return x.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d46470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pareto selection of Savitzky-Golay window size\n",
    "# Lorenz window range: 5 to 229, degree: 2\n",
    "# Burgers window range: 5 to 99, degree: 3\n",
    "def pareto_savgol_w(u_single):\n",
    "    # Initialization\n",
    "    windows = [i for i in range(5, 229, 2)]\n",
    "    solution_norms = []\n",
    "    residual_norms = []\n",
    "    \n",
    "    # Calculate norms for each window after filter application\n",
    "    for w in windows:\n",
    "        u_denoised = savgol_denoise_1D(u_single, w, 2)\n",
    "        residual = u_single - u_denoised\n",
    "        solution_norms.append(np.linalg.norm(u_denoised))\n",
    "        residual_norms.append(np.linalg.norm(residual))\n",
    "        \n",
    "    solution_norms = np.array(solution_norms)\n",
    "    residual_norms = np.array(residual_norms)\n",
    "    # Find and extract point of most curvature\n",
    "    curvature = np.gradient(np.gradient(solution_norms, residual_norms), \n",
    "                            residual_norms)\n",
    "    max_curvature_idx = np.argmax(np.abs(curvature))\n",
    "    optimal_w = windows[max_curvature_idx]\n",
    "    \n",
    "    # Return optimal window size\n",
    "    return optimal_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97effd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pareto selection of l1 Trend Filter lambda\n",
    "# Lorenz lambda range: 10^-5 to 10^2\n",
    "# Burgers lambda range: 10^-5 to 10^1\n",
    "def pareto_l1_lambda(u_single):\n",
    "    # Initialization\n",
    "    lambdas = np.logspace(-5, 2, 50)\n",
    "    solution_norms = []\n",
    "    residual_norms = []\n",
    "    \n",
    "    # Calculate norms for each window after filter application\n",
    "    for lambda_reg in lambdas:\n",
    "        u_denoised = l1_trend_filter(u_single, lambda_reg)\n",
    "        residual = u_single - u_denoised\n",
    "        solution_norms.append(np.linalg.norm(u_denoised))\n",
    "        residual_norms.append(np.linalg.norm(residual))\n",
    "        \n",
    "    solution_norms = np.array(solution_norms)\n",
    "    residual_norms = np.array(residual_norms)\n",
    "    # Find and extract point of most curvature\n",
    "    curvature = np.gradient(np.gradient(solution_norms, residual_norms), \n",
    "                            residual_norms)\n",
    "    max_curvature_idx = np.argmax(np.abs(curvature))\n",
    "    optimal_lambda = lambdas[max_curvature_idx]\n",
    "    \n",
    "    # Return optimal lambda\n",
    "    return optimal_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af85601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lorenz Main Test Loop\n",
    "# Add model printouts for correct equation extractions\n",
    "# SINDy setup support from [https://pysindy.readthedocs.io/en/latest/\n",
    "# examples/12_weakform_SINDy_examples/example.html#Test-weak-form-ODE-\n",
    "# functionality-on-Lorenz-equation]\n",
    "\n",
    "num_iterations = 500\n",
    "noise_levels = [0,5,10,15,20,25,30,35,40,45,50,55,60,65,70]\n",
    "true_coeffs_x = {'x': -10, 'y': 10}\n",
    "true_coeffs_y = {'x': 28, 'y': -1, 'xz': -1}\n",
    "true_coeffs_z = {'z': -2.666, 'xy': 1}\n",
    "threshold = 0.0005\n",
    "# Initialize training data\n",
    "dt = 0.001\n",
    "integrator_keywords = {}\n",
    "feature_names = ['x', 'y', 'z']\n",
    "t_train = np.arange(0, 8, dt)\n",
    "x0_train = [-8, 8, 27]\n",
    "t_train_span = (t_train[0], t_train[-1])\n",
    "x_train_original = solve_ivp(lorenz, t_train_span, x0_train, t_eval=t_train, \n",
    "                             **integrator_keywords).y.T\n",
    "\n",
    "for p in noise_levels: \n",
    "    # Initialize error metrics\n",
    "    success_count = 0\n",
    "    success_count_savgol = 0\n",
    "    success_count_l1 = 0\n",
    "    success_count_savl1 = 0\n",
    "    success_count_l1sav = 0\n",
    "    coeff_errors = []\n",
    "    coeff_errors_savgol = []\n",
    "    coeff_errors_l1 = []\n",
    "    coeff_errors_savl1 = []\n",
    "    coeff_errors_l1sav = []\n",
    "    wrong_terms_list = []\n",
    "    wrong_terms_savgol_list = []\n",
    "    wrong_terms_l1_list = []\n",
    "    wrong_terms_savl1_list = []\n",
    "    wrong_terms_l1sav_list = []\n",
    "    print(f\"Testing noise level: {p}\")\n",
    "    for _ in range(num_iterations):\n",
    "        # Add Gaussian noise corruption\n",
    "        x_train = add_noise(x_train_original, p)\n",
    "        # Initialize candidate library setup\n",
    "        library_functions = [lambda x: x, lambda x, y: x * y, lambda x: x ** 2]\n",
    "        library_function_names = [lambda x: x, lambda x, y: x + y, \n",
    "                                  lambda x: x + x]\n",
    "        ode_lib = ps.WeakPDELibrary(library_functions=library_functions, \n",
    "                                function_names=library_function_names, \n",
    "                                spatiotemporal_grid=t_train, \n",
    "                                is_uniform = True,\n",
    "                                K=100,\n",
    "                                include_bias=False) # no constant term\n",
    "        # No filter model\n",
    "        x_dot_integral = ode_lib.convert_u_dot_integral(x_train)\n",
    "        model = ps.SINDy(feature_names=feature_names,\n",
    "                    optimizer=ps.STLSQ(),\n",
    "                    feature_library=ode_lib)\n",
    "        model.fit(x_train, x_dot=x_dot_integral,t=dt)\n",
    "        #print(\"no filter\")\n",
    "        #model.print()\n",
    "        coeffs = model.coefficients()\n",
    "        # Filtering\n",
    "        num_vars = x_train.shape[1]\n",
    "        x_train_savgol = np.zeros_like(x_train)\n",
    "        x_train_l1 = np.zeros_like(x_train)\n",
    "        x_train_savl1 = np.zeros_like(x_train)\n",
    "        x_train_l1sav = np.zeros_like(x_train)\n",
    "        for i in range(num_vars):\n",
    "            x_pareto_w = lorenz_pareto_savgol_w(x_train[:, i])\n",
    "            x_train_savgol[:, i] = savgol_denoise_1D(x_train[:, i], \n",
    "                                                     x_pareto_w, 2)\n",
    "            x_pareto_lambda = lorenz_pareto_l1_lambda(x_train[:, i])\n",
    "            x_train_l1[:, i] = l1_trend_filter(x_train[:, i], x_pareto_lambda)\n",
    "        for i in range(num_vars):\n",
    "            savgol_pareto_lambda = lorenz_pareto_l1_lambda(x_train_savgol[:, i])\n",
    "            x_train_savl1[:, i] = l1_trend_filter(x_train_savgol[:, i], \n",
    "                                                  savgol_pareto_lambda)\n",
    "            l1_pareto_w = lorenz_pareto_savgol_w(x_train_l1[:, i])\n",
    "            x_train_l1sav[:, i] = savgol_denoise_1D(x_train_l1[:, i], \n",
    "                                                    l1_pareto_w, 2)\n",
    "        # Fitting filtered models\n",
    "        x_dot_integral_savgol = ode_lib.convert_u_dot_integral(x_train_savgol)    \n",
    "        x_dot_integral_l1 = ode_lib.convert_u_dot_integral(x_train_l1)    \n",
    "        x_dot_integral_savl1 = ode_lib.convert_u_dot_integral(x_train_savl1)  \n",
    "        x_dot_integral_l1sav = ode_lib.convert_u_dot_integral(x_train_l1sav)   \n",
    "        model_savgol = ps.SINDy(feature_names=feature_names,optimizer=ps.STLSQ(), \n",
    "                                feature_library=ode_lib) \n",
    "        model_savgol.fit(x_train_savgol, x_dot=x_dot_integral_savgol, t=dt)\n",
    "        coeffs_savgol = model_savgol.coefficients()\n",
    "        #print(\"savgol\")\n",
    "        #model_savgol.print()\n",
    "        model_l1 = ps.SINDy(feature_names=feature_names,optimizer=ps.STLSQ(), \n",
    "                            feature_library=ode_lib)\n",
    "        model_l1.fit(x_train_l1, x_dot=x_dot_integral_l1, t=dt)\n",
    "        coeffs_l1 = model_l1.coefficients()\n",
    "        #print(\"l1\")\n",
    "        #model_l1.print()\n",
    "        model_savl1 = ps.SINDy(feature_names=feature_names,optimizer=ps.STLSQ(), \n",
    "                               feature_library=ode_lib)\n",
    "        model_savl1.fit(x_train_savl1, x_dot=x_dot_integral_savl1, t=dt)\n",
    "        coeffs_savl1 = model_savl1.coefficients()\n",
    "        #print(\"savl1\")\n",
    "        #model_savl1.print()\n",
    "        model_l1sav = ps.SINDy(feature_names=feature_names,optimizer=ps.STLSQ(), \n",
    "                               feature_library=ode_lib)\n",
    "        model_l1sav.fit(x_train_l1sav, x_dot=x_dot_integral_l1sav, t=dt)\n",
    "        coeffs_l1sav = model_l1sav.coefficients()\n",
    "        #print(\"l1sav\")\n",
    "        #model_l1sav.print()\n",
    "        \n",
    "        timesteps = np.arange(x_train.shape[0])\n",
    "        labels = ['x', 'y', 'z']\n",
    "        all_terms = model.get_feature_names()\n",
    "        # Desired terms as per given equations\n",
    "        desired_terms_x = set(['x', 'y'])\n",
    "        desired_terms_y = set(['x', 'y', 'xz'])\n",
    "        desired_terms_z = set(['z', 'xy'])\n",
    "        # Helper function to get active terms for an equation\n",
    "        def get_active_terms(coeff_row, all_terms):\n",
    "            return set([term for idx, term in enumerate(all_terms) \n",
    "                        if abs(coeff_row[idx]) > threshold])\n",
    "        # Extract active terms\n",
    "        active_terms_x = get_active_terms(coeffs[0], all_terms)\n",
    "        active_terms_y = get_active_terms(coeffs[1], all_terms)\n",
    "        active_terms_z = get_active_terms(coeffs[2], all_terms)\n",
    "        active_terms_x_savgol = get_active_terms(coeffs_savgol[0], all_terms)\n",
    "        active_terms_y_savgol = get_active_terms(coeffs_savgol[1], all_terms)\n",
    "        active_terms_z_savgol = get_active_terms(coeffs_savgol[2], all_terms)\n",
    "        active_terms_x_l1 = get_active_terms(coeffs_l1[0], all_terms)\n",
    "        active_terms_y_l1 = get_active_terms(coeffs_l1[1], all_terms)\n",
    "        active_terms_z_l1 = get_active_terms(coeffs_l1[2], all_terms)\n",
    "        active_terms_x_savl1 = get_active_terms(coeffs_savl1[0], all_terms)\n",
    "        active_terms_y_savl1 = get_active_terms(coeffs_savl1[1], all_terms)\n",
    "        active_terms_z_savl1 = get_active_terms(coeffs_savl1[2], all_terms)\n",
    "        active_terms_x_l1sav = get_active_terms(coeffs_l1sav[0], all_terms)\n",
    "        active_terms_y_l1sav = get_active_terms(coeffs_l1sav[1], all_terms)\n",
    "        active_terms_z_l1sav = get_active_terms(coeffs_l1sav[2], all_terms)\n",
    "        # Success Rates\n",
    "        if (active_terms_x == desired_terms_x and \n",
    "            active_terms_y == desired_terms_y and \n",
    "            active_terms_z == desired_terms_z):\n",
    "            success_count += 1\n",
    "        if (active_terms_x_savgol == desired_terms_x and \n",
    "            active_terms_y_savgol == desired_terms_y and \n",
    "            active_terms_z_savgol == desired_terms_z):\n",
    "            success_count_savgol += 1\n",
    "        if (active_terms_x_l1 == desired_terms_x and \n",
    "            active_terms_y_l1 == desired_terms_y and \n",
    "            active_terms_z_l1 == desired_terms_z):\n",
    "            success_count_l1 += 1\n",
    "        if (active_terms_x_savl1 == desired_terms_x and \n",
    "            active_terms_y_savl1 == desired_terms_y and \n",
    "            active_terms_z_savl1 == desired_terms_z):\n",
    "            success_count_savl1 += 1\n",
    "        if (active_terms_x_l1sav == desired_terms_x and \n",
    "            active_terms_y_l1sav == desired_terms_y and \n",
    "            active_terms_z_l1sav == desired_terms_z):\n",
    "            success_count_l1sav += 1\n",
    "        # Coefficient Errors\n",
    "        coeff_error = 0\n",
    "        coeff_error_savgol = 0\n",
    "        coeff_error_l1 = 0\n",
    "        coeff_error_savl1 = 0\n",
    "        coeff_error_l1sav = 0\n",
    "        for term in desired_terms_x: \n",
    "            idx = all_terms.index(term)\n",
    "            coeff_error += abs(coeffs[0, idx] - true_coeffs_x[term])\n",
    "            coeff_error_savgol += abs(coeffs_savgol[0, idx] - true_coeffs_x[term])\n",
    "            coeff_error_l1 += abs(coeffs_l1[0, idx] - true_coeffs_x[term])\n",
    "            coeff_error_savl1 += abs(coeffs_savl1[0, idx] - true_coeffs_x[term])\n",
    "            coeff_error_l1sav += abs(coeffs_l1sav[0, idx] - true_coeffs_x[term])\n",
    "        for term in desired_terms_y: \n",
    "            idx = all_terms.index(term)\n",
    "            coeff_error += abs(coeffs[1, idx] - true_coeffs_y[term])\n",
    "            coeff_error_savgol += abs(coeffs_savgol[1, idx] - true_coeffs_y[term])\n",
    "            coeff_error_l1 += abs(coeffs_l1[1, idx] - true_coeffs_y[term])\n",
    "            coeff_error_savl1 += abs(coeffs_savl1[1, idx] - true_coeffs_y[term])\n",
    "            coeff_error_l1sav += abs(coeffs_l1sav[1, idx] - true_coeffs_y[term])\n",
    "        for term in desired_terms_z: \n",
    "            idx = all_terms.index(term)\n",
    "            coeff_error += abs(coeffs[2, idx] - true_coeffs_z[term])\n",
    "            coeff_error_savgol += abs(coeffs_savgol[2, idx] - true_coeffs_z[term])\n",
    "            coeff_error_l1 += abs(coeffs_l1[2, idx] - true_coeffs_z[term])\n",
    "            coeff_error_savl1 += abs(coeffs_savl1[2, idx] - true_coeffs_z[term])\n",
    "            coeff_error_l1sav += abs(coeffs_l1sav[2, idx] - true_coeffs_z[term])\n",
    "        coeff_errors.append(coeff_error)\n",
    "        coeff_errors_savgol.append(coeff_error_savgol)\n",
    "        coeff_errors_l1.append(coeff_error_l1)\n",
    "        coeff_errors_savl1.append(coeff_error_savl1)\n",
    "        coeff_errors_l1sav.append(coeff_error_l1sav)\n",
    "        # Wrong terms count\n",
    "        wrong_terms = 0\n",
    "        wrong_terms += len([term for term in active_terms_x if \n",
    "                            term not in desired_terms_x])\n",
    "        wrong_terms += len([term for term in active_terms_y if\n",
    "                            term not in desired_terms_y])\n",
    "        wrong_terms += len([term for term in active_terms_z if\n",
    "                            term not in desired_terms_z])\n",
    "        wrong_terms_savgol= 0\n",
    "        wrong_terms_savgol += len([term for term in active_terms_x_savgol if\n",
    "                                   term not in desired_terms_x])\n",
    "        wrong_terms_savgol += len([term for term in active_terms_y_savgol if\n",
    "                                   term not in desired_terms_y])\n",
    "        wrong_terms_savgol += len([term for term in active_terms_z_savgol if\n",
    "                                   term not in desired_terms_z])\n",
    "        wrong_terms_l1 = 0\n",
    "        wrong_terms_l1 += len([term for term in active_terms_x_l1 if\n",
    "                               term not in desired_terms_x])\n",
    "        wrong_terms_l1 += len([term for term in active_terms_y_l1 if\n",
    "                               term not in desired_terms_y])\n",
    "        wrong_terms_l1 += len([term for term in active_terms_z_l1 if\n",
    "                               term not in desired_terms_z])\n",
    "        wrong_terms_savl1 = 0\n",
    "        wrong_terms_savl1 += len([term for term in active_terms_x_savl1 if\n",
    "                                  term not in desired_terms_x])\n",
    "        wrong_terms_savl1 += len([term for term in active_terms_y_savl1 if\n",
    "                                  term not in desired_terms_y])\n",
    "        wrong_terms_savl1 += len([term for term in active_terms_z_savl1 if\n",
    "                                  term not in desired_terms_z])\n",
    "        wrong_terms_l1sav = 0\n",
    "        wrong_terms_l1sav += len([term for term in active_terms_x_l1sav if\n",
    "                                  term not in desired_terms_x])\n",
    "        wrong_terms_l1sav += len([term for term in active_terms_y_l1sav if\n",
    "                                  term not in desired_terms_y])\n",
    "        wrong_terms_l1sav += len([term for term in active_terms_z_l1sav if\n",
    "                                  term not in desired_terms_z])\n",
    "        wrong_terms_list.append(wrong_terms)\n",
    "        wrong_terms_savgol_list.append(wrong_terms_savgol)\n",
    "        wrong_terms_l1_list.append(wrong_terms_l1)\n",
    "        wrong_terms_savl1_list.append(wrong_terms_savl1)\n",
    "        wrong_terms_l1sav_list.append(wrong_terms_l1sav)\n",
    "    # Output metrics\n",
    "    success_rate = (success_count / num_iterations) * 100\n",
    "    success_rate_savgol = (success_count_savgol / num_iterations) * 100\n",
    "    success_rate_l1 = (success_count_l1 / num_iterations) * 100\n",
    "    success_rate_savl1 = (success_count_savl1 / num_iterations) * 100\n",
    "    success_rate_l1sav = (success_count_l1sav / num_iterations) * 100\n",
    "    print(f\"Success rate using original data: {success_rate}%\")\n",
    "    print(f\"Success rate using savgol data: {success_rate_savgol}%\")\n",
    "    print(f\"Success rate using l1 data: {success_rate_l1}%\")\n",
    "    print(f\"Success rate using savl1 data: {success_rate_savl1}%\")\n",
    "    print(f\"Success rate using l1sav data: {success_rate_l1sav}%\")\n",
    "    print(f\"Avg coeff error using original data: {np.mean(coeff_errors)}\")\n",
    "    print(f\"Avg coeff error using savgol data: {np.mean(coeff_errors_savgol)}\")\n",
    "    print(f\"Avg coeff error using l1 data: {np.mean(coeff_errors_l1)}\")\n",
    "    print(f\"Avg coeff error using savl1 data: {np.mean(coeff_errors_savl1)}\")\n",
    "    print(f\"Avg coeff error using l1sav data: {np.mean(coeff_errors_l1sav)}\")                                                                                  \n",
    "    print(f\"Avg # wrong terms using original data: {np.mean(wrong_terms_list)}\")\n",
    "    print(f\"Avg # wrong terms using savgol data: {np.mean(wrong_terms_savgol_list)}\")\n",
    "    print(f\"Avg # wrong terms using l1 data: {np.mean(wrong_terms_l1_list)}\")\n",
    "    print(f\"Avg # wrong terms using savl1 data: {np.mean(wrong_terms_savl1_list)}\")                     \n",
    "    print(f\"Avg # wrong terms using l1sav data: {np.mean(wrong_terms_l1sav_list)}\")                                                                                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d98a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viscous Burgers Main Test Loop\n",
    "# Add model printouts for correct equation extractions\n",
    "# SINDy setup support from [https://pysindy.readthedocs.io/en/latest/\n",
    "# examples/12_weakform_SINDy_examples/example.html#Test-weak-form-PDE-\n",
    "# functionality-on-Burgers'-equation-with-20%-noise]\n",
    "\n",
    "num_iterations = 500\n",
    "noise_levels = [0,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100]\n",
    "threshold = 0.0005\n",
    "true_coeffs = {'x0_11': 0.1, 'x0x0_1': -1.0}\n",
    "\n",
    "for p in noise_levels: \n",
    "    # Initialize error metrics\n",
    "    success_count = 0\n",
    "    success_count_savgol = 0\n",
    "    success_count_l1 = 0\n",
    "    success_count_savl1 = 0\n",
    "    success_count_l1sav = 0\n",
    "    coeff_errors = []\n",
    "    coeff_errors_savgol = []\n",
    "    coeff_errors_l1 = []\n",
    "    coeff_errors_savl1 = []\n",
    "    coeff_errors_l1sav = []\n",
    "    wrong_terms_list = []\n",
    "    wrong_terms_savgol_list = []\n",
    "    wrong_terms_l1_list = []\n",
    "    wrong_terms_savl1_list = []\n",
    "    wrong_terms_l1sav_list = []\n",
    "    print(f\"Testing noise level: {p}\")\n",
    "    for _ in range(num_iterations):\n",
    "        # Load data file\n",
    "        data = loadmat(\"data/burgers.mat\")\n",
    "        time = np.ravel(data[\"t\"])\n",
    "        x = np.ravel(data[\"x\"])\n",
    "        u = np.real(data[\"usol\"])\n",
    "        dt = time[1] - time[0]\n",
    "        dx = x[1] - x[0]\n",
    "        # Add Gaussian noise and reshape\n",
    "        u = add_noise(u, p)\n",
    "        u = np.reshape(u, (len(x), len(time), 1))\n",
    "        # Initialize candidate library setup\n",
    "        library_functions = [lambda x: x, lambda x: x * x]\n",
    "        library_function_names = [lambda x: x, lambda x: x + x]\n",
    "        # Define 2D spatiotemporal grid \n",
    "        X, T = np.meshgrid(x, time)\n",
    "        XT = np.asarray([X, T]).T\n",
    "        pde_lib = ps.WeakPDELibrary(\n",
    "            library_functions=library_functions,\n",
    "            function_names=library_function_names,\n",
    "            derivative_order=2,\n",
    "            spatiotemporal_grid=XT,\n",
    "            is_uniform=True,\n",
    "            K=1000,\n",
    "        )\n",
    "        # Filtering\n",
    "        u_savgol = np.empty_like(u)\n",
    "        u_l1 = np.empty_like(u)\n",
    "        u_savl1 = np.empty_like(u)\n",
    "        u_l1sav = np.empty_like(u)\n",
    "        u_pareto_lambda = burgers_pareto_l1_lambda(u)\n",
    "        u_pareto_w = burgers_pareto_savgol_w(u)\n",
    "        for j in range(u.shape[1]):\n",
    "            u_savgol[:, j, 0] = savgol_denoise_1D(u[:, j, 0], u_pareto_w, 3)\n",
    "        for j in range(u.shape[1]):\n",
    "            u_l1[:, j, 0] = l1_trend_filter(u[:, j, 0], u_pareto_lambda)\n",
    "        savgol_pareto_lambda = burgers_pareto_l1_lambda(u_savgol)\n",
    "        l1_pareto_w = burgers_pareto_savgol_w(u_l1)\n",
    "        for j in range(u.shape[1]):\n",
    "            u_savl1[:, j, 0] = l1_trend_filter(u_savgol[:, j, 0], savgol_pareto_lambda)\n",
    "        for j in range(u.shape[1]):\n",
    "            u_l1sav[:,j,0] = savgol_denoise_1D(u_l1[:, j, 0], l1_pareto_w, 3)\n",
    "        # Fit all models\n",
    "        optimizer = ps.SR3(\n",
    "            threshold=0.1, thresholder=\"l0\", tol=1e-8, \n",
    "            normalize_columns=True, max_iter=1000\n",
    "        )   \n",
    "        model = ps.SINDy(feature_library=pde_lib, optimizer=optimizer)\n",
    "        model_savgol = ps.SINDy(feature_library=pde_lib, optimizer=optimizer)\n",
    "        model_l1 = ps.SINDy(feature_library=pde_lib, optimizer=optimizer)\n",
    "        model_savl1 = ps.SINDy(feature_library=pde_lib, optimizer=optimizer)\n",
    "        model_l1sav = ps.SINDy(feature_library=pde_lib, optimizer=optimizer)\n",
    "        model.fit(u, quiet=True)\n",
    "        coeffs = model.coefficients()\n",
    "        #print(\"no filter\")\n",
    "        #model.print()\n",
    "        model_savgol.fit(u_savgol, quiet=True)\n",
    "        coeffs_savgol = model_savgol.coefficients()\n",
    "        #print(\"savgol\")\n",
    "        #model_savgol.print()\n",
    "        model_l1.fit(u_l1, quiet=True)\n",
    "        coeffs_l1 = model_l1.coefficients()\n",
    "        #print(\"l1\")\n",
    "        #model_l1.print()\n",
    "        model_savl1.fit(u_savl1, quiet=True)\n",
    "        coeffs_savl1 = model_savl1.coefficients()\n",
    "        #print(\"savl1\")\n",
    "        #model_savl1.print()\n",
    "        model_l1sav.fit(u_l1sav, quiet=True)\n",
    "        coeffs_l1sav = model_l1sav.coefficients()\n",
    "        #print(\"l1sav\")\n",
    "        #model_l1sav.print()\n",
    "        # Extract active terms\n",
    "        all_terms = model.get_feature_names()\n",
    "        non_zero_indices = np.where(np.abs(coeffs) > threshold)[1]\n",
    "        active_terms = [all_terms[i] for i in non_zero_indices]\n",
    "        non_zero_indices_savgol = np.where(np.abs(coeffs_savgol) > threshold)[1]\n",
    "        active_terms_savgol = [all_terms[i] for i in non_zero_indices_savgol]\n",
    "        non_zero_indices_l1 = np.where(np.abs(coeffs_l1) > threshold)[1]\n",
    "        active_terms_l1 = [all_terms[i] for i in non_zero_indices_l1]\n",
    "        non_zero_indices_savl1 = np.where(np.abs(coeffs_savl1) > threshold)[1]\n",
    "        active_terms_savl1 = [all_terms[i] for i in non_zero_indices_savl1]\n",
    "        non_zero_indices_l1sav = np.where(np.abs(coeffs_l1sav) > threshold)[1]\n",
    "        active_terms_l1sav = [all_terms[i] for i in non_zero_indices_l1sav]\n",
    "        desired_terms = set(['x0_11', 'x0x0_1'])\n",
    "        # Success Rate\n",
    "        if set(active_terms) == desired_terms:\n",
    "            success_count += 1\n",
    "        if set(active_terms_savgol) == desired_terms:\n",
    "            success_count_savgol += 1   \n",
    "        if set(active_terms_l1) == desired_terms:\n",
    "            success_count_l1 += 1   \n",
    "        if set(active_terms_savl1) == desired_terms:\n",
    "            success_count_savl1 += 1   \n",
    "        if set(active_terms_l1sav) == desired_terms:\n",
    "            success_count_l1sav += 1   \n",
    "        # Coefficient Errors\n",
    "        coeff_error = 0\n",
    "        coeff_error_savgol = 0\n",
    "        coeff_error_l1 = 0\n",
    "        coeff_error_savl1 = 0\n",
    "        coeff_error_l1sav = 0\n",
    "        for term in desired_terms:\n",
    "            idx = all_terms.index(term)\n",
    "            coeff_error += abs(coeffs[0, idx] - true_coeffs[term])\n",
    "            coeff_error_savgol += abs(coeffs_savgol[0, idx] - true_coeffs[term])\n",
    "            coeff_error_l1 += abs(coeffs_l1[0, idx] - true_coeffs[term])\n",
    "            coeff_error_savl1 += abs(coeffs_savl1[0, idx] - true_coeffs[term])\n",
    "            coeff_error_l1sav += abs(coeffs_l1sav[0, idx] - true_coeffs[term])\n",
    "        coeff_errors.append(coeff_error)\n",
    "        coeff_errors_savgol.append(coeff_error_savgol)\n",
    "        coeff_errors_l1.append(coeff_error_l1)\n",
    "        coeff_errors_savl1.append(coeff_error_savl1)\n",
    "        coeff_errors_l1sav.append(coeff_error_l1sav)\n",
    "        # Wrong terms count\n",
    "        wrong_terms = [term for term in active_terms if\n",
    "                       term not in desired_terms]\n",
    "        wrong_terms_savgol = [term for term in active_terms_savgol if\n",
    "                              term not in desired_terms]\n",
    "        wrong_terms_l1 = [term for term in active_terms_l1 if\n",
    "                          term not in desired_terms]\n",
    "        wrong_terms_savl1 = [term for term in active_terms_savl1 if\n",
    "                             term not in desired_terms]\n",
    "        wrong_terms_l1sav = [term for term in active_terms_l1sav if\n",
    "                             term not in desired_terms]\n",
    "        wrong_terms_list.append(len(wrong_terms))\n",
    "        wrong_terms_savgol_list.append(len(wrong_terms_savgol))\n",
    "        wrong_terms_l1_list.append(len(wrong_terms_l1))\n",
    "        wrong_terms_savl1_list.append(len(wrong_terms_savl1))\n",
    "        wrong_terms_l1sav_list.append(len(wrong_terms_l1sav))\n",
    "    # Output metrics\n",
    "    success_rate = (success_count / num_iterations) * 100\n",
    "    success_rate_savgol = (success_count_savgol / num_iterations) * 100\n",
    "    success_rate_l1 = (success_count_l1 / num_iterations) * 100\n",
    "    success_rate_savl1 = (success_count_savl1 / num_iterations) * 100\n",
    "    success_rate_l1sav = (success_count_l1sav / num_iterations) * 100\n",
    "    print(f\"Success rate using original data: {success_rate}%\")\n",
    "    print(f\"Success rate using savgol data: {success_rate_savgol}%\")\n",
    "    print(f\"Success rate using l1 data: {success_rate_l1}%\")\n",
    "    print(f\"Success rate using savl1 data: {success_rate_savl1}%\")\n",
    "    print(f\"Success rate using l1sav data: {success_rate_l1sav}%\")\n",
    "    print(f\"Avg coeff error using original data: {np.mean(coeff_errors)}\")\n",
    "    print(f\"Avg coeff error using savgol data: {np.mean(coeff_errors_savgol)}\")\n",
    "    print(f\"Avg coeff error using l1 data: {np.mean(coeff_errors_l1)}\")\n",
    "    print(f\"Avg coeff error using savl1 data: {np.mean(coeff_errors_savl1)}\")\n",
    "    print(f\"Avg coeff error using l1sav data: {np.mean(coeff_errors_l1sav)}\")                                                                   \n",
    "    print(f\"Avg # wrong terms using original data: {np.mean(wrong_terms_list)}\")\n",
    "    print(f\"Avg # wrong terms using savgol data: {np.mean(wrong_terms_savgol_list)}\")\n",
    "    print(f\"Avg # wrong terms using l1 data: {np.mean(wrong_terms_l1_list)}\")\n",
    "    print(f\"Avg # wrong terms using savl1 data: {np.mean(wrong_terms_savl1_list)}\") \n",
    "    print(f\"Avg # wrong terms using l1sav data: {np.mean(wrong_terms_l1sav_list)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
