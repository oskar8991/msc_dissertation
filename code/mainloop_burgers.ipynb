{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae016b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "import pysindy as ps\n",
    "from scipy.signal import savgol_filter\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "from scipy.optimize import minimize\n",
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd50775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadmat(\"data/burgers.mat\")\n",
    "time = np.ravel(data[\"t\"])\n",
    "x = np.ravel(data[\"x\"])\n",
    "u = np.real(data[\"usol\"])\n",
    "dt = time[1] - time[0]\n",
    "dx = x[1] - x[0]\n",
    "#u_noise = add_noise(u, 20)\n",
    "#u_noise = np.reshape(u, (len(x), len(time), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b325e308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(data, percentage=0):\n",
    "    rmse = mean_squared_error(u, np.zeros(data.shape), squared=False)\n",
    "    data = data + np.random.normal(0, (percentage/100)*rmse, u.shape)\n",
    "    return data\n",
    "\n",
    "def savgol_denoise_1D(data, window_length=11,poly_order=3):\n",
    "    denoised_data = savgol_filter(data, window_length, poly_order)  \n",
    "    return denoised_data\n",
    "\n",
    "def l1_trend_filter(y, lamb=0):\n",
    "    n = len(y)\n",
    "    x = cp.Variable(n)\n",
    "    # Construct 2nd order difference matrix D2\n",
    "    D = np.diff(np.eye(n), axis=0)\n",
    "    D2 = np.diff(D, axis=0)\n",
    "    # Define the objective\n",
    "    objective = cp.Minimize(0.5 * cp.norm(x - y, 2)**2 + lamb * cp.norm(D2 @ x, 1))\n",
    "    # Solve the problem\n",
    "    problem = cp.Problem(objective)\n",
    "    # Try using another solver\n",
    "    try:\n",
    "        problem.solve()\n",
    "    except cp.SolverError:\n",
    "        try:\n",
    "            problem.solve(solver=cp.SCS)\n",
    "        except cp.SolverError:\n",
    "            print(\"SCS also failed, trying another solver...\")\n",
    "            problem.solve(solver=cp.OSQP)\n",
    "    return x.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e6f96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def burgers_pareto_l1_lambda(u):\n",
    "    lambdas = np.logspace(-5, 1, 50)\n",
    "    solution_norms = []\n",
    "    residual_norms = []\n",
    "\n",
    "    for lambda_reg in lambdas:\n",
    "        u_denoised = np.zeros_like(u)\n",
    "        # Apply L1 Trend Filter\n",
    "        for j in range(u.shape[1]):\n",
    "            u_denoised[:, j, 0] = l1_trend_filter(u[:, j, 0], lambda_reg)\n",
    "        residual = u - u_denoised\n",
    "        solution_norms.append(np.linalg.norm(u_denoised))\n",
    "        residual_norms.append(np.linalg.norm(residual))\n",
    "    \n",
    "    solution_norms = np.array(solution_norms)\n",
    "    residual_norms = np.array(residual_norms)\n",
    "    curvature = np.gradient(np.gradient(solution_norms, residual_norms), residual_norms)\n",
    "    max_curvature_idx = np.argmax(np.abs(curvature))\n",
    "    optimal_lambda = lambdas[max_curvature_idx]\n",
    "    \n",
    "    return optimal_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0999592b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def burgers_pareto_savgol_w(u):\n",
    "    windows = [i for i in range(7, 99, 2)]\n",
    "    solution_norms = []\n",
    "    residual_norms = []\n",
    "\n",
    "    for w in windows:\n",
    "        u_denoised = np.zeros_like(u)\n",
    "        # Apply savgol filter\n",
    "        for j in range(u.shape[1]):\n",
    "            u_denoised[:, j, 0] = savgol_denoise_1D(u[:, j, 0], w, 3)\n",
    "        residual = u - u_denoised\n",
    "        solution_norms.append(np.linalg.norm(u_denoised))\n",
    "        residual_norms.append(np.linalg.norm(residual))\n",
    "    \n",
    "    solution_norms = np.array(solution_norms)\n",
    "    residual_norms = np.array(residual_norms)\n",
    "    curvature = np.gradient(np.gradient(solution_norms, residual_norms), residual_norms)\n",
    "    max_curvature_idx = np.argmax(np.abs(curvature))\n",
    "    optimal_w = windows[max_curvature_idx]\n",
    "    \n",
    "    return optimal_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffb20da",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 500\n",
    "\n",
    "noise_levels = [0,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100]\n",
    "threshold = 0.0005\n",
    "\n",
    "true_coeffs = {'x0_11': 0.1, 'x0x0_1': -1.0}\n",
    "\n",
    "for p in noise_levels: \n",
    "    \n",
    "    success_count = 0\n",
    "    success_count_savgol = 0\n",
    "    success_count_l1 = 0\n",
    "    success_count_savl1 = 0\n",
    "    success_count_l1sav = 0\n",
    "    \n",
    "    coeff_errors = []\n",
    "    coeff_errors_savgol = []\n",
    "    coeff_errors_l1 = []\n",
    "    coeff_errors_savl1 = []\n",
    "    coeff_errors_l1sav = []\n",
    "    \n",
    "    wrong_terms_list = []\n",
    "    wrong_terms_savgol_list = []\n",
    "    wrong_terms_l1_list = []\n",
    "    wrong_terms_savl1_list = []\n",
    "    wrong_terms_l1sav_list = []\n",
    "    \n",
    "    print(f\"Testing noise level: {p}\")\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        \n",
    "        data = loadmat(\"data/burgers.mat\")\n",
    "        time = np.ravel(data[\"t\"])\n",
    "        x = np.ravel(data[\"x\"])\n",
    "        u = np.real(data[\"usol\"])\n",
    "        dt = time[1] - time[0]\n",
    "        dx = x[1] - x[0]\n",
    "\n",
    "        u = add_noise(u, p)\n",
    "        u = np.reshape(u, (len(x), len(time), 1))\n",
    "        \n",
    "        # Define weak form PDE library\n",
    "        library_functions = [lambda x: x, lambda x: x * x]\n",
    "        library_function_names = [lambda x: x, lambda x: x + x]\n",
    "\n",
    "        # Need to define the 2D spatiotemporal grid before calling the library\n",
    "        X, T = np.meshgrid(x, time)\n",
    "        XT = np.asarray([X, T]).T\n",
    "        pde_lib = ps.WeakPDELibrary(\n",
    "            library_functions=library_functions,\n",
    "            function_names=library_function_names,\n",
    "            derivative_order=2,\n",
    "            spatiotemporal_grid=XT,\n",
    "            is_uniform=True,\n",
    "            K=1000,\n",
    "        )\n",
    "        \n",
    "        u_savgol = np.empty_like(u)\n",
    "        u_l1 = np.empty_like(u)\n",
    "        u_savl1 = np.empty_like(u)\n",
    "        u_l1sav = np.empty_like(u)\n",
    "        \n",
    "        u_pareto_lambda = burgers_pareto_l1_lambda(u)\n",
    "        u_pareto_w = burgers_pareto_savgol_w(u)\n",
    "    \n",
    "        # Apply Savitzky-Golay filter\n",
    "        for j in range(u.shape[1]):\n",
    "            u_savgol[:, j, 0] = savgol_denoise_1D(u[:, j, 0], u_pareto_w, 3)\n",
    "\n",
    "        # Apply L1 Trend Filter\n",
    "        for j in range(u.shape[1]):\n",
    "            u_l1[:, j, 0] = l1_trend_filter(u[:, j, 0], u_pareto_lambda)\n",
    "   \n",
    "        savgol_pareto_lambda = burgers_pareto_l1_lambda(u_savgol)\n",
    "        l1_pareto_w = burgers_pareto_savgol_w(u_l1)\n",
    "        #print(f\"savgol_pareto_lambda: {savgol_pareto_lambda}\")\n",
    "        # Apply combined Savitzky-Golay and L1 Trend Filter (both directions)\n",
    "        for j in range(u.shape[1]):\n",
    "            u_savl1[:, j, 0] = l1_trend_filter(u_savgol[:, j, 0], savgol_pareto_lambda)\n",
    "        for j in range(u.shape[1]):\n",
    "            u_l1sav[:,j,0] = savgol_denoise_1D(u_l1[:, j, 0], l1_pareto_w, 3)\n",
    "            \n",
    "               \n",
    "        optimizer = ps.SR3(\n",
    "            threshold=0.1, thresholder=\"l0\", tol=1e-8, normalize_columns=True, max_iter=1000\n",
    "        )   \n",
    "        \n",
    "        model = ps.SINDy(feature_library=pde_lib, optimizer=optimizer)\n",
    "        model_savgol = ps.SINDy(feature_library=pde_lib, optimizer=optimizer)\n",
    "        model_l1 = ps.SINDy(feature_library=pde_lib, optimizer=optimizer)\n",
    "        model_savl1 = ps.SINDy(feature_library=pde_lib, optimizer=optimizer)\n",
    "        model_l1sav = ps.SINDy(feature_library=pde_lib, optimizer=optimizer)\n",
    "        \n",
    "        model.fit(u, quiet=True)\n",
    "        coeffs = model.coefficients()\n",
    "        #print(\"no filter\")\n",
    "        #model.print()\n",
    "        model_savgol.fit(u_savgol, quiet=True)\n",
    "        coeffs_savgol = model_savgol.coefficients()\n",
    "        #print(\"savgol\")\n",
    "        #model_savgol.print()\n",
    "        model_l1.fit(u_l1, quiet=True)\n",
    "        coeffs_l1 = model_l1.coefficients()\n",
    "        #print(\"l1\")\n",
    "        #model_l1.print()\n",
    "        model_savl1.fit(u_savl1, quiet=True)\n",
    "        coeffs_savl1 = model_savl1.coefficients()\n",
    "        #print(\"savl1\")\n",
    "        #model_savl1.print()\n",
    "        model_l1sav.fit(u_l1sav, quiet=True)\n",
    "        coeffs_l1sav = model_l1sav.coefficients()\n",
    "        #print(\"l1sav\")\n",
    "        #model_l1sav.print()\n",
    "        \n",
    "        all_terms = model.get_feature_names()\n",
    "\n",
    "        # For the original model\n",
    "        non_zero_indices = np.where(np.abs(coeffs) > threshold)[1] # the [1] is used since coeffs is 2D\n",
    "        active_terms = [all_terms[i] for i in non_zero_indices]\n",
    "        # For the savgol model\n",
    "        non_zero_indices_savgol = np.where(np.abs(coeffs_savgol) > threshold)[1]\n",
    "        active_terms_savgol = [all_terms[i] for i in non_zero_indices_savgol]\n",
    "        # For the tikhonov model\n",
    "        non_zero_indices_l1 = np.where(np.abs(coeffs_l1) > threshold)[1]\n",
    "        active_terms_l1 = [all_terms[i] for i in non_zero_indices_l1]\n",
    "        # For the double model\n",
    "        non_zero_indices_savl1 = np.where(np.abs(coeffs_savl1) > threshold)[1]\n",
    "        active_terms_savl1 = [all_terms[i] for i in non_zero_indices_savl1]\n",
    "        non_zero_indices_l1sav = np.where(np.abs(coeffs_l1sav) > threshold)[1]\n",
    "        active_terms_l1sav = [all_terms[i] for i in non_zero_indices_l1sav]\n",
    "        \n",
    "\n",
    "        desired_terms = set(['x0_11', 'x0x0_1'])\n",
    "\n",
    "        # Check if the model's active terms match the desired terms\n",
    "        if set(active_terms) == desired_terms:\n",
    "            success_count += 1\n",
    "        if set(active_terms_savgol) == desired_terms:\n",
    "            success_count_savgol += 1   \n",
    "        if set(active_terms_l1) == desired_terms:\n",
    "            success_count_l1 += 1   \n",
    "        if set(active_terms_savl1) == desired_terms:\n",
    "            success_count_savl1 += 1   \n",
    "        if set(active_terms_l1sav) == desired_terms:\n",
    "            success_count_l1sav += 1   \n",
    "            \n",
    "        \n",
    "        # Coefficient Errors\n",
    "        coeff_error = 0\n",
    "        coeff_error_savgol = 0\n",
    "        coeff_error_l1 = 0\n",
    "        coeff_error_savl1 = 0\n",
    "        coeff_error_l1sav = 0\n",
    "        for term in desired_terms:\n",
    "            idx = all_terms.index(term)\n",
    "            coeff_error += abs(coeffs[0, idx] - true_coeffs[term])\n",
    "            coeff_error_savgol += abs(coeffs_savgol[0, idx] - true_coeffs[term])\n",
    "            coeff_error_l1 += abs(coeffs_l1[0, idx] - true_coeffs[term])\n",
    "            coeff_error_savl1 += abs(coeffs_savl1[0, idx] - true_coeffs[term])\n",
    "            coeff_error_l1sav += abs(coeffs_l1sav[0, idx] - true_coeffs[term])\n",
    "\n",
    "        coeff_errors.append(coeff_error)\n",
    "        coeff_errors_savgol.append(coeff_error_savgol)\n",
    "        coeff_errors_l1.append(coeff_error_l1)\n",
    "        coeff_errors_savl1.append(coeff_error_savl1)\n",
    "        coeff_errors_l1sav.append(coeff_error_l1sav)\n",
    "        \n",
    "        # Wrong terms count\n",
    "        wrong_terms = [term for term in active_terms if term not in desired_terms]\n",
    "        wrong_terms_savgol = [term for term in active_terms_savgol if term not in desired_terms]\n",
    "        wrong_terms_l1 = [term for term in active_terms_l1 if term not in desired_terms]\n",
    "        wrong_terms_savl1 = [term for term in active_terms_savl1 if term not in desired_terms]\n",
    "        wrong_terms_l1sav = [term for term in active_terms_l1sav if term not in desired_terms]\n",
    "\n",
    "        wrong_terms_list.append(len(wrong_terms))\n",
    "        wrong_terms_savgol_list.append(len(wrong_terms_savgol))\n",
    "        wrong_terms_l1_list.append(len(wrong_terms_l1))\n",
    "        wrong_terms_savl1_list.append(len(wrong_terms_savl1))\n",
    "        wrong_terms_l1sav_list.append(len(wrong_terms_l1sav))\n",
    "            \n",
    "            \n",
    "    # Compute average and standard deviation for each metric\n",
    "    # Compute success rates\n",
    "    success_rate = (success_count / num_iterations) * 100\n",
    "    success_rate_savgol = (success_count_savgol / num_iterations) * 100\n",
    "    success_rate_l1 = (success_count_l1 / num_iterations) * 100\n",
    "    success_rate_savl1 = (success_count_savl1 / num_iterations) * 100\n",
    "    success_rate_l1sav = (success_count_l1sav / num_iterations) * 100\n",
    "\n",
    "    print(f\"Success rate using original data: {success_rate}%\")\n",
    "    print(f\"Success rate using savgol data: {success_rate_savgol}%\")\n",
    "    print(f\"Success rate using l1 data: {success_rate_l1}%\")\n",
    "    print(f\"Success rate using savl1 data: {success_rate_savl1}%\")\n",
    "    print(f\"Success rate using l1sav data: {success_rate_l1sav}%\")\n",
    "    \n",
    "    print(f\"Average coefficient error using original data: {np.mean(coeff_errors)} ± {np.std(coeff_errors)}\")\n",
    "    print(f\"Average coefficient error using savgol data: {np.mean(coeff_errors_savgol)} ± {np.std(coeff_errors_savgol)}\")\n",
    "    print(f\"Average coefficient error using l1 data: {np.mean(coeff_errors_l1)} ± {np.std(coeff_errors_l1)}\")\n",
    "    print(f\"Average coefficient error using savl1 data: {np.mean(coeff_errors_savl1)} ± {np.std(coeff_errors_savl1)}\")\n",
    "    print(f\"Average coefficient error using l1sav data: {np.mean(coeff_errors_l1sav)} ± {np.std(coeff_errors_l1sav)}\")\n",
    "                                                                                          \n",
    "    print(f\"Average number of wrong terms using original data: {np.mean(wrong_terms_list)} ± {np.std(wrong_terms_list)}\")\n",
    "    print(f\"Average number of wrong terms using savgol data: {np.mean(wrong_terms_savgol_list)} ± {np.std(wrong_terms_savgol_list)}\")\n",
    "    print(f\"Average number of wrong terms using l1 data: {np.mean(wrong_terms_l1_list)} ± {np.std(wrong_terms_l1_list)}\")\n",
    "    print(f\"Average number of wrong terms using savl1 data: {np.mean(wrong_terms_savl1_list)} ± {np.std(wrong_terms_savl1_list)}\") \n",
    "    print(f\"Average number of wrong terms using l1sav data: {np.mean(wrong_terms_l1sav_list)} ± {np.std(wrong_terms_l1sav_list)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
